{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Network\n",
    "\n",
    "In this jupyter notebook we will focus on $\\textit{Neural Netowrks}$ (also called MultiLayer Perceptrons), which is computing system, that is based on biological neural networks that constitute animal brains. \n",
    "\n",
    "<img src=\"Neural_net/neural_net.jpg\" height=\"40%\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "For a given set $D = \\{(x_n, y_n)\\}_{n=1}^N$ classify $\\textit{y}$ for the new $\\textbf{x}$. To do it we will have to teach our neural network how to predict the class basing on input data. \n",
    "\n",
    "<h3>Example</h3>\n",
    "Given a dataset from MNIST (set of handwritten digits) we have to predict proper digit according to the input image:\n",
    "<img src=\"Neural_net/example.jpg\" height=\"70%\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example\n",
    "We want our model to learn if the output is ture of false, according to input data that will represent the xor logical gate, with A and B as inputs and O as output. The xor is a logical gate that gives a true output, when number of true inputs is odd.\n",
    "<img src=\"Neural_net/xor.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "\n",
    "Neural network have to learn itself what weights it has set to the inputs, so it can easliy decide if the output is true or not. For example it can develop weights to detect features like this:\n",
    "\n",
    "<img src=\"Neural_net/neural_net_xor.jpeg\" height=\"70%\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "We have to prepare some input data, for example four pairs A, B like in table above, but we will store it in just single variable x, and the output O in variable y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "    Y = np.array([[0],[1],[1],[0]])\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (4, 2)\n",
      "Output shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_data()\n",
    "print('Input shape: {}'.format(X.shape))\n",
    "print('Output shape: {}'.format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "Our model is a simple neural network with one hidden layer. That consists of input layer, where the \"x\" is given, hidden layer, and output layer, where we predict 'y'. In each layer there is addidtional node, which is not connected with the previous layer called bias. This neuron allows us to provide some independency between inputs.\n",
    "\n",
    "<img src=\"Neural_net/neural_net_bias.jpg\" height=\"40%\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "We are going to use sigmoid non-linear function, that is defined:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "And looks like this below:\n",
    "<img src=\"Neural_net/sigmoid.jpg\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "We also need to know sigmoid derivative which is equal to:\n",
    "$\\frac{\\delta \\sigma}{\\delta x} = \\sigma(x) \\cdot (1 - \\sigma(x)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    param x: input matrix of size NxM\n",
    "    '''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid for small value f.e. -100 equals: 0.000\n",
      "Sigmoid of big values f.e. 100 equals: 1.000\n"
     ]
    }
   ],
   "source": [
    "print('Sigmoid for small value f.e. -100 equals: {:.3f}'.format(sigmoid(-100)))\n",
    "print('Sigmoid of big values f.e. 100 equals: {:.3f}'.format(sigmoid(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "Now we are going to implement steps:\n",
    "1. We are multiplying inputs by weights and adding bias to get $z_1$ values at the hidden layer.\n",
    "3. On the $z_1$ we are applaying non-linear function to 'activate' the layer.\n",
    "4. We are multiplying values from the hidden layer by next weights and adding another bias to get $z_2$ value on output layer\n",
    "6. At the end we need to apply another non-linear function on output layer.\n",
    "\n",
    "$z_1 = (a_0 \\times w_1) + b_1 $\n",
    "\n",
    "$a_1 = \\sigma(z_1) $\n",
    "\n",
    "$z_2 = (a_1 \\times w_2 ) + b_2 $\n",
    "\n",
    "$a_2 = \\sigma(z_2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, w, b):\n",
    "    '''\n",
    "    param x: vector of input training values NxM, where N is the number of examples & M number of features\n",
    "    param w: tuple of two matrixes, which describes weights between layers\n",
    "    param b: tuple of two vectors, which describes biases for layers\n",
    "    returns: function returns cache, which is tuple, that includes values of activated layers.\n",
    "    '''\n",
    "    w1, w2 = w # unpacking tuple to single matrixes\n",
    "    b1, b2 = b # unpacking tuple to single vectors\n",
    "    \n",
    "    a0 = x\n",
    "    \n",
    "    z1 = (a0 @ w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = (a1 @ w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    cache = (a1, a2)\n",
    "    \n",
    "    return cache    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "At start weights are random numbers, so we need to find parameters that will predict output with the smallest error. we are going to search for them using gradient descent, which is a method that allows to reach local minimum by substracting derivatives values from weights.\n",
    "\n",
    "Before computing the gradient we need to define cost function, which in our case will be mean squared error: \n",
    "$J = \\frac{1}{2N}\\lVert{\\textbf{y}} - \\overline{\\textbf{y}}\\rVert_2^2$\n",
    "\n",
    "The cost function will let us measure how bad our model works. \n",
    "\n",
    "\n",
    "$\\lVert \\textbf{y} - \\overline{\\textbf{y}}\\rVert_2$ is called Norm 2 <br>\n",
    "In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space\n",
    "<br>\n",
    "\n",
    "$\\lVert\\textbf{x}\\rVert_2 = \\sqrt[2]{\\sum_{n=1}^{N} x_n^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_weights(inputs_amount, hidden_nodes_amount, outputs_amount):\n",
    "    '''\n",
    "    param inputs_amount: number of cells in input layer\n",
    "    param output_amount: number of cells in output layer \n",
    "    return: pair of tuples of weights and biases for proper layers\n",
    "    '''\n",
    "    np.random.seed(100)\n",
    "\n",
    "    w1 = np.random.normal(0, 1 / np.sqrt(hidden_nodes_amount), (inputs_amount, hidden_nodes_amount))\n",
    "    b1 = np.random.normal(0, 1, hidden_nodes_amount)\n",
    "\n",
    "    w2 = np.random.normal(0, 1 / np.sqrt(outputs_amount), (hidden_nodes_amount, outputs_amount))\n",
    "    b2 = np.random.normal(0, 1, outputs_amount)\n",
    "\n",
    "    return (w1, w2), (b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (from input layer to hidden layer) shape: (2, 3)\n",
      "b1 (bias added to hidden layer) shape: (3,)\n",
      "W2 (from hidden layer to outputs layer) shape: (3, 1)\n",
      "b2 (bias added to outplut layer) shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "w, b = initialize_random_weights(inputs_amount = 2, hidden_nodes_amount = 3, outputs_amount=1)\n",
    "\n",
    "w1, w2 = w\n",
    "b1, b2 = b\n",
    "\n",
    "print(\"W1 (from input layer to hidden layer) shape: {}\".format(w1.shape))\n",
    "print(\"b1 (bias added to hidden layer) shape: {}\".format(b1.shape))\n",
    "print(\"W2 (from hidden layer to outputs layer) shape: {}\".format(w2.shape))\n",
    "print(\"b2 (bias added to outplut layer) shape: {}\".format(b2.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "To make some improvements to our weights we need to compute the error, and then following the chain rule change the parameters as follows:\n",
    "<h3>Steps of gradeint descent</h3>\n",
    "1. Compute the cost (the error on our last layer)\n",
    "2. Compute the error on not activated values $z_2$\n",
    "3. Compute the gradient on weights $w_2$ and bias $b_2$\n",
    "4. Compute the error on layer $a_1$\n",
    "5. Compute the error on not activated values $z_1$\n",
    "6. compute the gradient on weights $w_1$ and bias $b_1$ \n",
    "\n",
    "<h3> How are we going to compute those errors</h3>\n",
    "We need to compute derivative of our cost function with coressponding to our weights and biases, so after substracting computed values from weights and biases we become closer to minimum of cost function.\n",
    "<p><i>When the cost function grows the derivative is positive, so substracting it will make a 'step back' in direction of  minimum. \n",
    "\n",
    "When the function decreases the derivative is negative, so substracting it will make a 'step forward' in direction of  minimum.\n",
    "\n",
    "If this is hard to imagine - draw yourself a $x^2$ function and place yourself on the left and right of the minimum, compute derivative and substract it from x.\n",
    "</i></p>\n",
    "\n",
    "<h4>Detailed steps of gradient descent using chain rule (for people familiar with calculus) </h4>\n",
    "1. Compute the cost: $ J = \\frac{1}{2N}\\lVert{\\textbf{y} - \\textbf{a}_2}\\rVert_2^2$\n",
    "\n",
    "2. Compute the derivative of cost with respect to  $ a_2 = \\space \\frac{\\delta J}{\\delta a_2} = -2 \\cdot(y - a_2) $\n",
    "\n",
    "3. Compute the derivative of cost with respect to $ z_2 = \\space \\frac{\\delta J}{\\delta z_2} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} = \\frac{\\delta J}{\\delta a_2} \\cdot (a_2 \\cdot (1 -a_2)) \\space,$because  $ \\frac{\\delta a_2}{\\delta z_2} = a_2 \\cdot (1-a_2) \\space $ as $a_2$ is the $\\sigma(z_2)$\n",
    "\n",
    "4. 1. Compute the derivative of cost with respect to $ w_2 = \\space \\frac{\\delta J}{\\delta w_2} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta w_2}  =\\frac{\\delta J}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta w_2} =  \\frac{\\delta J}{\\delta z_2} \\cdot a_1 \\space$, beacuse $\\frac{\\delta z_2}{\\delta w_2} = a_1 \\space$ as $z_2 = a_1 \\cdot w_2 + b_2 $\n",
    "\n",
    " 2. Compute the derivative of cost with respect to $ b_2 = \\space \\frac{\\delta J}{\\delta b_2} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta b_2}  =\\frac{\\delta J}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta b_2} =  \\frac{\\delta J}{\\delta z_2} \\cdot 1 \\space$, beacuse $\\frac{\\delta z_2}{\\delta b_2} = 1 \\space$ as $z_2 = a_1 \\cdot w_2 + b_2 $\n",
    " 3. Compute the derivative of cost with respect to  $ a_1 = \\space \\frac{\\delta J}{\\delta a_1} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta a_1}  =\\frac{\\delta J}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta a_1} =  \\frac{\\delta J}{\\delta z_2} \\cdot w_2 \\space$, beacuse $\\frac{\\delta z_2}{\\delta b_2} = w_2 \\space$ as $z_2 = a_1 \\cdot w_2 + b_2 $\n",
    "5. Compute the derivative of cost with respect to $z_1  =\\space \\frac{\\delta J}{\\delta z_1} =\\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta a_1} \\cdot \\frac{\\delta a_1}{\\delta z_1} = \\frac{\\delta J}{\\delta a_1} \\cdot \\frac{\\delta a_1}{\\delta z_1} = \\frac{\\delta J}{\\delta a_1} \\cdot (a_1 \\cdot ( 1 - a_1 )), \\space $because $\\frac{\\delta a_1}{\\delta z_1} = a_1 \\cdot ( 1 - a_1 )$ as $a_1 = \\sigma(z_1)$\n",
    "6. 1. Compute the derivative of cost with respect to $w_1 =\\space \\frac{\\delta J}{\\delta w_1} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta a_1} \\cdot \\frac{\\delta a_1}{\\delta z_1} \\cdot \\frac{\\delta z_1}{\\delta w_1} = \\frac{\\delta J}{\\delta z_1} \\cdot \\frac{\\delta z_1}{\\delta w_1} = \\frac{\\delta J}{\\delta z_1} \\cdot a_0, \\space $ beacuse $\\frac{\\delta z_1}{\\delta w_1} = a_0, \\space$ as $z_1 = a_0 \\cdot w_1 + b_1$\n",
    "  2. Compute the derivative of cost with respect to $b_1 =\\space \\frac{\\delta J}{\\delta b_1} = \\frac{\\delta J}{\\delta a_2} \\cdot \\frac{\\delta a_2}{\\delta z_2} \\cdot \\frac{\\delta z_2}{\\delta a_1} \\cdot \\frac{\\delta a_1}{\\delta z_1} \\cdot \\frac{\\delta z_1}{\\delta b_1} = \\frac{\\delta J}{\\delta z_1} \\cdot \\frac{\\delta z_1}{\\delta b_1} = \\frac{\\delta J}{\\delta z_1} \\cdot 1, \\space $ because $\\frac{\\delta z_1}{\\delta b_1} = 1, \\space$ as $z_1 = a_0 \\cdot w_1 + b_1$\n",
    "\n",
    "We don't need to calculate our derivative with respect to a_0, as this is our input, which we are not going to change. \n",
    "Now, when we have our derivatives computed, or rather <b>gradients</b>, as this are operations on matrices. \n",
    "In code those operations may bit a lit differ, as we are operating on matrices and sometimes you need to transpose them or compute it in proper order, to make it right. (In matrices $A \\times B \\neq B \\times A$) And we are going to compute the mean gradient over all training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, w, b, y, cache):\n",
    "    '''\n",
    "    param x: input matrix of size NxM\n",
    "    param w: tuple of weights\n",
    "    param b: tuple of biases\n",
    "    param y: matrix of size NxO\n",
    "    param cache: tuple of values in layers a1,a2 from forward propagation\n",
    "    '''\n",
    "    w1, w2 = w  # unpacking tuple to single matrixes\n",
    "    b1, b2 = b  # unpacking tuple to single vectors\n",
    "\n",
    "    a1, a2 = cache\n",
    "\n",
    "    N = y.shape[0]\n",
    "\n",
    "    a0 = x\n",
    "\n",
    "    dJ_da2 = -2 *(y - a2)\n",
    "    dJ_dz2 =  dJ_da2 * a2*(1-a2)\n",
    "\n",
    "    dJ_dw2 = 1/N * a1.T @ dJ_dz2\n",
    "    dJ_db2 = np.mean(dJ_dz2 * 1, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ w2.T\n",
    "    dJ_dz1 = dJ_da1 * a1 * (1-a1)\n",
    "\n",
    "    dJ_dw1 = 1/N * a0.T @ dJ_dz1\n",
    "    dJ_db1 = np.mean(dJ_dz1 * 1, axis=0)\n",
    "\n",
    "\n",
    "    cache = (dJ_dw1, dJ_dw2), (dJ_db1, dJ_db2)\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "<h3>Steps for training</h3>\n",
    "After backpropagation we can update weights and biases by substracting from them those gradients multiplied by hyperparamter called learning rate, which defines how fast our model is learning itself. \n",
    "\n",
    "$w_2 := w_2 - learning\\_rate \\cdot \\frac{\\delta J}{\\delta w_2}$\n",
    "\n",
    "$b_2 := b_2 - learning\\_rate \\cdot \\frac{\\delta J}{\\delta b_2}$\n",
    "\n",
    "$w_1 := w_1 - learning\\_rate \\cdot \\frac{\\delta J}{\\delta w_1}$\n",
    "\n",
    "$b_1 := b_1 - learning\\_rate \\cdot \\frac{\\delta J}{\\delta b_1}$\n",
    "\n",
    "  0 - Initialize weights and biases with random values\n",
    "\n",
    "For k times (where k is number of epochs)\n",
    "\n",
    " 1. Process forward pass to get values at layers\n",
    " 2. Process backward pass to get error at weighst and biases\n",
    " 3. Update weights and biases to be closer to Cost function minimum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, hidden_nodes_amount = 2, epochs = 10000, learning_rate = 1e-3):\n",
    "    '''\n",
    "    param x: input data of shape NxM\n",
    "    \n",
    "    param y: output data of shape NxO\n",
    "    \n",
    "    param hidden_nodes_amount: number of nodes in hidden layer\n",
    "    \n",
    "    param epochs: nubmer of epochs\n",
    "    \n",
    "    param learning_rate: rate of learning speed\n",
    "    \n",
    "    return: funcion return weights, biases, costs\n",
    "    '''\n",
    "    \n",
    "    inputs_amount = x.shape[1]\n",
    "    \n",
    "    outputs_amount = y.shape[1]\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    \n",
    "    costs=[]\n",
    "    \n",
    "    w, b = initialize_random_weights(inputs_amount, hidden_nodes_amount, outputs_amount)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        #cache is (a1,a2), values in layers\n",
    "        cache = forward_pass(x, w, b)\n",
    "        _, a2 = cache\n",
    "        \n",
    "        (dw1, dw2), (db1, db2) = backward_pass(x, w, b, y, cache)\n",
    "        \n",
    "        cost = np.mean((y-a2)**2/2)\n",
    "        \n",
    "        costs.append(cost)\n",
    "\n",
    "        w1, w2 = w\n",
    "        b1, b2 = b\n",
    "\n",
    "        w1 -= dw1\n",
    "        w2 -= dw2\n",
    "\n",
    "        b1 -= db1\n",
    "        b2 -= db2\n",
    "\n",
    "        w = (w1, w2)\n",
    "        b = (b1, b2)\n",
    "\n",
    "    return w,b,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Prediction is a single feed forward part for each input, to generate classified output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    '''\n",
    "    param x: input matrix NxM\n",
    "    param w: tuple of weights\n",
    "    param b: tuple of biases\n",
    "    param threshold_value: decision border\n",
    "    '''\n",
    "    w1, w2 = w\n",
    "    b1, b2 = b\n",
    "\n",
    "\n",
    "    _, result = reforward_pass(x, w, b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b9595393f44e9a8303b26a083c738a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reforward_pass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-923546c5de11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input: {}, Real output: {}, Prediction: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-bc0b958c1c3f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(x, w, b)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reforward_pass' is not defined"
     ]
    }
   ],
   "source": [
    "w, b, _ = train(x=X, y=Y)\n",
    "prediction = predict(X, w, b)\n",
    "for i in range(prediction.shape[0]):\n",
    "    print('Input: {}, Real output: {}, Prediction: {}'.format(X[i], Y[i], prediction[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complicated classification task - MNIST Dataset\n",
    "Mnist is a popular database of handwritten images created for people who are new to machine learning. There are many courses on the internet that include classification problem using MNIST dataset.\n",
    "\n",
    "This dataset contains 55000 images and labels. Each image is 28x28 pixels large, but for the purpose of the classification task they are flattened to 784x1 arrays $(28 \\cdot 28 = 784)$. Summing up our training set is a matrix of size $[55000, 784]$  = [amount of images, size of image]. Each label is size of $[10, 1]$, beacuse it is in 'one-hot' format. In this format we are using only 'zeros' and single 'one' to represent a number. For instance:\n",
    "\n",
    "$3_{10} = \\begin{bmatrix}0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix}_{one\\_hot}$ \n",
    "\n",
    "$7_{10} = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}_{one\\_hot}$\n",
    "\n",
    "It is done to ease neural network learning. \n",
    "\n",
    "It also contains 5000 test images and labels, which are used for testing purpose. We are going to test results on diffrent set than training to avoid overfitting and get \"true\" accuracy on new inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Mnist Data Example</h3>\n",
    "<img src=\"Neural_net/mnist_example.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "Now we are going to download this dataset and split it into test and train sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test = utils.get_mnist_dataset()\n",
    "\n",
    "train_images, train_labels = train_data\n",
    "test_images, test_labels = test\n",
    "\n",
    "\n",
    "print(\"Training images matrix size: {}\".format(train_images.shape))\n",
    "print(\"Training labels matrix size: {}\".format(train_labels.shape))\n",
    "\n",
    "print(\"Testing images matrix size: {}\".format(test_images.shape))\n",
    "print(\"Testing labels matrix size: {}\".format(test_labels.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "Visualisation isn't necessery to the problem, but it helps to understand what are we doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_few(images):\n",
    "    '''\n",
    "    param images: vecotr of images of size 28x28 to plot \n",
    "    '''\n",
    "    ax =[]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    gs = GridSpec(2, 4, wspace=0.0, hspace=-0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            ax.append(fig.add_subplot(gs[i,j]))\n",
    "    for i, axis in enumerate(ax):\n",
    "        axis.imshow(images[i])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_8_images = train_images[:8]\n",
    "resized = np.reshape(first_8_images, (-1,28,28))\n",
    "print('First 8 images of train data:')\n",
    "show_few(resized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some intuition\n",
    "\n",
    "Without neural network you have to perform following steps:\n",
    "1. Extract features from image\n",
    "2. Choose and inplement the model\n",
    "3. Choose and implement learning algorithm\n",
    "4. (Learn model if necessary) and predict class for the new image with this model\n",
    "\n",
    "In neural network you can skip some of those, as network learns how to extract features by itslef in hidden layers. You need only to specify its size and hyperparameters (learning rate, regularization, etc.)\n",
    "<img src=\"Neural_net/neural_intuition.jpg\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "To provide some more intuition about how is neural network perforing binary classification and extracting features there is another example below:\n",
    "<img src=\"Neural_net/neural_intuition2.jpg\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutliclass classification\n",
    "In the previous problem we had only 2 classes - True or False in the last node. It was an example of binary classification. In MNIST task we have 10 classes, so in the ouptput layer we need 10 neurons. \n",
    "<img src=\"Neural_net/neural_one_hot.jpg\" height=\"75%\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy counting\n",
    "As our model is never returning 1 or 0, due to activation function sigmoid on output layer, we have to find maximum argument, to detect class predicted by network. \n",
    "\n",
    "<h3>Example</h3>\n",
    "$[0.2 ;\\space  0.123; \\space 0.5] \\stackrel{arg\\_max}{\\implies} [0;\\space 0;\\space 1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_accuracy(prediction, y):\n",
    "    '''\n",
    "    param prediction: the output of neural network\n",
    "    param y: the true output values\n",
    "    '''\n",
    "    hits = np.argmax(prediction, axis=1) == np.argmax(y, axis = 1)\n",
    "    \n",
    "    accuracy = np.mean(hits, axis = 0)\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, costs = train(x=train_images, y=train_labels, hidden_nodes_amount = 100, epochs=600, learning_rate=5e-3)\n",
    "prediction = predict(test_images, w, b)\n",
    "\n",
    "accuracy = count_accuracy(prediction, test_labels)\n",
    "print(\"Accuracy {:.3f} %\".format(accuracy * 100))\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Cost value')\n",
    "plt.title('Cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Accuracy on mnist dataset should be about 90% which is quite poor. To get better results you are supposed to add another hidden layers, or even better - use diffrent types of neural networks like Convolutional Neural Networks. \n",
    "\n",
    "If you are intrested in machine learning topic we recommend:\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning - great introduction to machine learning by Prof. Andrew Ng, unfortunately the course is written in matlab, but provides necessary software. If you don't want to work in matlab, you can still get theoretical background.\n",
    "\n",
    "https://www.coursera.org/specializations/deep-learning - a next step into deep learning with more details, prepared as well by Prof. Andrew Ng, written in python. \n",
    "\n",
    "https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw - a great youtube channel where you can gain essential knowledge of math - linear algebra, calculus and other stuff - especially neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "Following sources were used to create this notebook:\n",
    "\n",
    "http://mlg.ii.pwr.edu.pl/sz/#/courses - Wrocław University of Science and Technology, Szymon Zaręba Ph.D. - neural network intuintion first image \n",
    "\n",
    "https://think-data.github.io/machine%20learning/python/2016/12/03/simple-neural-network-using-tensorflow.html - mnist network visualisation with one-hot output layer\n",
    "\n",
    "https://www.youtube.com/watch?v=BR9h47Jtqyw - neural net intuintion second image (feature extraction)\n",
    "\n",
    "https://becominghuman.ai/neural-network-xor-application-and-fundamentals-6b1d539941ed - the XOR neural network image\n",
    "\n",
    "http://www.vlsiinterviewquestions.org/2012/04/17/xor-gate-using-21-mux/ - xor logical gate with table image\n",
    "\n",
    "https://db-blog.web.cern.ch/ - first example image\n",
    "\n",
    "http://web.stanford.edu/class/cs20si/syllabus.html - Stanford computer science course with presentation about deep learning\n",
    "\n",
    "https://www.coursera.org/learn/nlp-sequence-models/notebook/acNYU/emojify - one_hot function from utils\n",
    "\n",
    "https://www.kaggle.com/pablotab/mnistpklgz - mnist dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
